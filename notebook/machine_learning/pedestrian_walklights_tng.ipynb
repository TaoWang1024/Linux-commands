{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ffeb037-5575-4fd6-a40e-51078e9fe0bd",
   "metadata": {},
   "source": [
    "from: <i><b style='color:red;'>grokking</b> <b>Deep Learning</b></i>\n",
    "<p>by Andrew W. Trask</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdf00b-2921-406a-9404-36b8f13b8ef2",
   "metadata": {},
   "source": [
    "<p><b>prerequisites</b></p>\n",
    "$$f(x) = \\mathbf{w^{*}x} + b^{*},$$\n",
    "where <b>w*</b> and b* are optimal values for parameters <b>w</b> and b\n",
    "<p></p>\n",
    "<p>perceptron</p>\n",
    "<p>gradient descent</p>\n",
    "<p>backpropagation</p>\n",
    "<p><b>\"The interface for the neural network is simple: it accepts an <i>input</i> variable as information and a <i>weights</i> variable as knowledge, and it outputs a prediction.\"</b></p>\n",
    "<p><b>\"Measuring error simplifies the problem of training neural networks to make correct predictions.\"</b></p>\n",
    "<p><b>\"Different ways of measuring the error prioritize error differently.\"</b></p>\n",
    "<p>Error is calculated and applied to modify the weights during each iteration of the training.</p>\n",
    "<p><b>\"<i>alpha</i> is the simplest way to prevent overcorrecting weight updates.\"</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002d571-a584-431a-a5e6-9615b58fac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98129caf-5db8-4877-a964-63775b48b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization, functions\n",
    "\n",
    "def gradient_descent(prediction, target):\n",
    "    ''' One method for calculating error.\n",
    "    '''\n",
    "    return (prediction - target)**2\n",
    "\n",
    "def gradient_descent_deriv(weights):\n",
    "    ''' Taking the derivative of the error\n",
    "        during training\n",
    "        yields amount and direction of the prediction\n",
    "        from the target.\n",
    "    '''\n",
    "    return (2 * weights - 1)\n",
    "    \n",
    "def relu(x):\n",
    "    ''' Returns x iff x > 0; otherwise, returns 0\n",
    "    '''\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    ''' Returns 1 for input > 0; otherwise, returns 0\n",
    "    '''\n",
    "    return output > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1d4fc-d285-4ec2-a68b-e23705bc8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target\n",
    "streetlights = np.array([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]).reshape(4, 3) # layer 0 (input) & \"x\" in layer 1\n",
    "walk_v_stop = np.array([1, 1, 0, 0]).T # values to train the model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267534fd-3f53-40a9-b9be-75a174a4463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.2 # scale down correction to prevent overcorrection\n",
    "hidden_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8862d4b-e9dc-47fa-8f79-422fe48def5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization, weights\n",
    "weights_0_1 = gradient_descent_deriv(np.random.random((3, hidden_size)))\n",
    "weights_1_2 = gradient_descent_deriv(np.random.random((hidden_size, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4454ef-3de1-457e-8d1c-f6819afeb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for iteration in range(300):\n",
    "    ''' supervised learning\n",
    "    '''\n",
    "    # reset layer_2_error to 0\n",
    "    layer_2_error = 0\n",
    "\n",
    "    for index, values in enumerate(streetlights):\n",
    "        layer_0 = streetlights[index:index+1] # rename input\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "        # difference between layer 2 output and predicted values\n",
    "        layer_2_error += np.sum(gradient_descent(layer_2, walk_v_stop[index:index+1]))\n",
    "\n",
    "        # calculate the correction\n",
    "        layer_2_delta = (layer_2 - walk_v_stop[index:index+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        # apply the correction --- note that corrected weights are running sums\n",
    "        # alpha is a fractional value to dampen correction, preventing overcorrection\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if (iteration % 10 == 9):\n",
    "        print(f\"Error: {layer_2_error:.25f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc9ef3-2775-469e-b569-ac8bf7d36e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b87fe-c511-4ae7-a55f-eeca131b76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickle/weights_0_1.pickle.bin', 'wb') as out_file:\n",
    "    pickle.dump(weights_0_1, out_file)\n",
    "with open('pickle/weights_1_2.pickle.bin', 'wb') as out_file:\n",
    "    pickle.dump(weights_1_2, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
