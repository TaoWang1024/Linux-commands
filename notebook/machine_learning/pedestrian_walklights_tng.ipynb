{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25fb1ca-eaa7-4ad7-959a-f00490cdab1d",
   "metadata": {},
   "source": [
    "neural network example from: <i><b style='color:red;'>grokking</b> <b>Deep Learning</b></i>\n",
    "<p>by Andrew W. Trask</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff033ba-65ee-4bc6-90bb-8942bc3c55dd",
   "metadata": {},
   "source": [
    "<img src=\"images/chollet_change_of_paradigm.jpg\" width=400 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129753c-a048-4a81-ad81-f1b4a7adcea5",
   "metadata": {},
   "source": [
    "<p><i>Machine learning: A New Programming Paradigm</i></p>\n",
    "<p>from <i>Chollet, François</i>. <b>Deep Learning With Python</b>. pg 4. Manning Publications, 2021.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acfcb0-a56c-4ab2-8610-8f2c01a09dee",
   "metadata": {},
   "source": [
    "<p><b>prerequisites</b></p>\n",
    "$$f(x) = \\mathbf{w^{*}x} + b^{*},$$\n",
    "where <b>w*</b> and b* are optimal values for parameters <b>w</b> and b\n",
    "<p></p>\n",
    "<p>layer</p>\n",
    "<p>weight</p>\n",
    "<p>optimizer: used to update the weights throughout the neural network. gradient descent, which uses y=x**2</p>\n",
    "<p>activation functions: relu: REctified Linear Unit, a nonlinear function that allows a two-layer neural network to be a universal function approximator</p>\n",
    "<p>backpropagation</p>: both the optimizer and the activation functions must be differentiable.\n",
    "<p></p>\n",
    "<p><b>\"The interface for the neural network is simple: it accepts an <i>input</i> variable as information and a <i>weights</i> variable as knowledge, and it outputs a prediction.\"</b></p>\n",
    "<p><b>\"Measuring error simplifies the problem of training neural networks to make correct predictions.\"</b></p>\n",
    "<p><b>\"Different ways of measuring the error prioritize error differently.\"</b></p>\n",
    "<p>Error is calculated and applied to modify the weights during each iteration of the training.</p>\n",
    "<p><b>\"<i>alpha</i> is the simplest way to prevent overcorrecting weight updates.\"</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0900acf-f454-48e4-8c35-4cafd0c36bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot\" width=\"800\" height=\"565\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot\" width=\"800\" height=\"565\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c002d571-a584-431a-a5e6-9615b58fac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, random, dot, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98129caf-5db8-4877-a964-63775b48b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization, functions\n",
    "\n",
    "def gradient_descent(prediction, target):\n",
    "    ''' Optimization function.\n",
    "        One method for calculating error.\n",
    "    '''\n",
    "    return (prediction - target)**2\n",
    "\n",
    "def gradient_descent_deriv(weights):\n",
    "    ''' Taking the derivative of the error\n",
    "        during training\n",
    "        yields amount and direction of the prediction\n",
    "        from the target.\n",
    "    '''\n",
    "    return (2 * weights - 1)\n",
    "    \n",
    "def relu(x):\n",
    "    ''' Activation function.\n",
    "        Returns x iff x > 0; otherwise, returns 0\n",
    "    '''\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    ''' Returns 1 for input > 0; otherwise, returns 0\n",
    "    '''\n",
    "    return output > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c1d4fc-d285-4ec2-a68b-e23705bc8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and target\n",
    "streetlights = array([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1]).reshape(4, 3) # layer 0 (input) & \"x\" in layer 1\n",
    "walk_v_stop = array([1, 1, 0, 0]).T # values to train the model on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6699ed12-19c3-4925-ae4e-0d571f00f2e2",
   "metadata": {},
   "source": [
    "<img src=\"images/chollet_data_moving_through_a_neural_network.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8846569-241f-4ded-bb1c-2c1a939b2166",
   "metadata": {},
   "source": [
    "<p><i>Data representations learned by a digit-classification model</i>: As data moves through a neural network (NN), it is transformed as it combines with weights between the layers.</p>\n",
    "<p>Hence, the data of input layer 0 is combined with the weights between layers 0 and 1, to become the data in layer 1.</p>\n",
    "<p>This combining of data with weights proceeds through the neural network, until the transformed data reaches the output layer.</p>\n",
    "<p>Training a neural network, simplified: the network's output is compared to desired output, the difference is quantified; and the quantification is used to update the values of the weights throughout the network. This is done again and again, until the output matches the desired output. At this point, the network is considered trained.</p>\n",
    "<p>from <i>Chollet, François</i>. <b>Deep Learning With Python</b>. pg 8. Manning Publications, 2021.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042c620-07c5-4668-a9d2-3fd12cfc8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.2 # scale down correction to prevent overcorrection\n",
    "hidden_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8862d4b-e9dc-47fa-8f79-422fe48def5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization, weights\n",
    "weights_0_1 = gradient_descent_deriv(random.random((3, hidden_size)))\n",
    "weights_1_2 = gradient_descent_deriv(random.random((hidden_size, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c029d62-3bbd-4d4a-9d2f-32ed8a7cf938",
   "metadata": {},
   "source": [
    "<img src=\"images/chollet_understanding_how_deep_learning_works_fig_3.jpg\" width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76333516-bc2c-4cad-8149-070ecfebbb0a",
   "metadata": {},
   "source": [
    "<p><i>Understanding How Deep Learning Works</i>: A deep neural network (DNN) is made up of representation or transformation layers. Modifiable weights tune layer values. Input data is propagated forward through the network to a final layer, where it yields a prediction.</p>\n",
    "<p>In a learning cycle, the error of the prediction is quantified by a loss function, an optimizer function then generates updates that are backpropagated through the network to the weights, tuning them.</p>\n",
    "<p>The cycle is repeated: input forward propagates through the network, yielding a new prediction, its error is quantified; new updates to the weights in the network are back propagated.</p>\n",
    "<p>from <i>Chollet, François</i>. <b>Deep Learning With Python</b>. pp 9-10. Manning Publications, 2021.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4454ef-3de1-457e-8d1c-f6819afeb47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.8901101359929014522620605\n",
      "Error: 0.2542225526616996300610651\n",
      "Error: 0.0186000876466763456762443\n",
      "Error: 0.0062291689519348037579194\n",
      "Error: 0.0016339736738625557522370\n",
      "Error: 0.0006586111523007657760423\n",
      "Error: 0.0002500491859843846490820\n",
      "Error: 0.0001085322721131527133826\n",
      "Error: 0.0000410147964808285335763\n",
      "Error: 0.0000159449822260412228897\n",
      "Error: 0.0000087309193676741935169\n",
      "Error: 0.0000026306292082643411198\n",
      "Error: 0.0000010738070527644372341\n",
      "Error: 0.0000004353914817750722659\n",
      "Error: 0.0000001683515648137285689\n",
      "Error: 0.0000000682748026573587427\n",
      "Error: 0.0000000278075228277859034\n",
      "Error: 0.0000000107485813911973141\n",
      "Error: 0.0000000043550284016043672\n",
      "Error: 0.0000000017680519885896441\n",
      "Error: 0.0000000007208188479200446\n",
      "Error: 0.0000000002780996995221337\n",
      "Error: 0.0000000001128993979392229\n",
      "Error: 0.0000000000457882830866893\n",
      "Error: 0.0000000000177632954110868\n",
      "Error: 0.0000000000072109707022017\n",
      "Error: 0.0000000000029218254338904\n",
      "Error: 0.0000000000011338711933542\n",
      "Error: 0.0000000000005919147719123\n",
      "Error: 0.0000000000001864004804111\n"
     ]
    }
   ],
   "source": [
    "# TNG\n",
    "for iteration in range(300):\n",
    "    ''' supervised learning\n",
    "        streetlights are the input; walk_v_stop are the labels (desired output)\n",
    "    '''\n",
    "    # reset layer_2_error to 0\n",
    "    layer_2_error = 0\n",
    "\n",
    "    for index, values in enumerate(streetlights):\n",
    "        # because layer_1 & 2 are calculated using the weights,\n",
    "        # and the weights get modified each iteration,\n",
    "        # the layers must be recalculated\n",
    "        layer_0 = streetlights[index:index+1] # INPUT\n",
    "        layer_1 = relu(dot(layer_0, weights_0_1)) # layer_0: (1, 3), weights_0_1: (3, 4); output: (1, 4)\n",
    "        layer_2 = dot(layer_1, weights_1_2) # layer_1: (1, 4), weights_1_2: (4, 1); MODEL OUTPUT: scalar\n",
    "        ''' the layers are interconnected.\n",
    "            algebraically, we can restate the last two lines as:\n",
    "            layer_2 = relu(layer_0.dot(weights_0_1)).dot(weights_1_2)\n",
    "        '''\n",
    "\n",
    "        # difference between layer 2 output and predicted values\n",
    "        # for human consumption, to be printed at the end of the iteration\n",
    "        layer_2_error += sum(gradient_descent(layer_2, walk_v_stop[index:index+1]))\n",
    "\n",
    "        # calculate the correction\n",
    "        layer_2_delta = (layer_2 - walk_v_stop[index:index+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) # backpropagation\n",
    "\n",
    "        # apply the correction --- note that corrected weights are running sums\n",
    "        # alpha is a fractional value to dampen correction, preventing overcorrection\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if (iteration % 10 == 9):\n",
    "        print(f\"Error: {layer_2_error:.25f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72dc9ef3-2775-469e-b569-ac8bf7d36e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type                          Data/Info\n",
      "----------------------------------------------------------------\n",
      "alpha                    float                         0.2\n",
      "array                    builtin_function_or_method    <built-in function array>\n",
      "dot                      _ArrayFunctionDispatcher      <built-in function dot>\n",
      "gradient_descent         function                      <function gradient_descent at 0x7e82d1fa0900>\n",
      "gradient_descent_deriv   function                      <function gradient_descen<...>_deriv at 0x7e82d1fa09a0>\n",
      "hidden_size              int                           4\n",
      "index                    int                           3\n",
      "iteration                int                           299\n",
      "layer_0                  ndarray                       1x3: 3 elems, type `int64`, 24 bytes\n",
      "layer_1                  ndarray                       1x4: 4 elems, type `float64`, 32 bytes\n",
      "layer_1_delta            ndarray                       1x4: 4 elems, type `float64`, 32 bytes\n",
      "layer_2                  ndarray                       1x1: 1 elems, type `float64`, 8 bytes\n",
      "layer_2_delta            ndarray                       1x1: 1 elems, type `float64`, 8 bytes\n",
      "layer_2_error            float64                       1.8640048041110218e-13\n",
      "random                   module                        <module 'numpy.random' fr<...>umpy/random/__init__.py'>\n",
      "relu                     function                      <function relu at 0x7e82d1fa0a40>\n",
      "relu2deriv               function                      <function relu2deriv at 0x7e82d1fa0ae0>\n",
      "streetlights             ndarray                       4x3: 12 elems, type `int64`, 96 bytes\n",
      "sum                      _ArrayFunctionDispatcher      <function sum at 0x7e82e81014e0>\n",
      "values                   ndarray                       3: 3 elems, type `int64`, 24 bytes\n",
      "walk_v_stop              ndarray                       4: 4 elems, type `int64`, 32 bytes\n",
      "weights_0_1              ndarray                       3x4: 12 elems, type `float64`, 96 bytes\n",
      "weights_1_2              ndarray                       4x1: 4 elems, type `float64`, 32 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bde83c7-56db-4d5e-be26-edf9e90984e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(layer_0, weights_0_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b87fe-c511-4ae7-a55f-eeca131b76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickle/weights_0_1.pickle.bin', 'wb') as out_file:\n",
    "    pickle.dump(weights_0_1, out_file)\n",
    "with open('pickle/weights_1_2.pickle.bin', 'wb') as out_file:\n",
    "    pickle.dump(weights_1_2, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
